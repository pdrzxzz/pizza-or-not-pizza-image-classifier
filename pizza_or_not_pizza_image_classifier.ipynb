{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1waFqTmfkJT_o-2iZppKwi0CYi9sMx5w3",
      "authorship_tag": "ABX9TyOKBy1FSaYv37zMZUiEj607",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pdrzxzz/pizza-or-not-pizza-image-classifier/blob/main/pizza_or_not_pizza_image_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install kaggle\n",
        "# !pip install albumentations"
      ],
      "metadata": {
        "id": "_3kxgpGyj7sA"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Ezz6iuAvr2Lr"
      },
      "outputs": [],
      "source": [
        "# ============================== #\n",
        "#         Import Libraries       #\n",
        "# ============================== #\n",
        "\n",
        "# Essential libraries for machine learning\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Image processing from albumentations (Image augmentation)\n",
        "import albumentations as A\n",
        "\n",
        "# Image processing from skimage\n",
        "from skimage import img_as_ubyte # Convert grayscale image to unsigned 8-bit format (required by LBP)\n",
        "from skimage.io import imread  # To read image files\n",
        "from skimage.transform import resize  # To resize images\n",
        "from skimage.feature import local_binary_pattern  # To extract texture\n",
        "from skimage import filters # sobel, prewitt... # To extract edges\n",
        "\n",
        "# Scikit-learn Data processing\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "\n",
        "# Scikit-learn modules for training and evaluation\n",
        "from sklearn.model_selection import train_test_split, learning_curve, StratifiedKFold, GridSearchCV, cross_val_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.base import clone\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "\n",
        "# Utilities\n",
        "import kagglehub  # To download datasets directly from Kaggle using KaggleHub\n",
        "import os  # For interacting with the operating system, such as file paths\n",
        "import joblib # Save models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================== #\n",
        "#        Download Dataset        #\n",
        "# ============================== #\n",
        "\n",
        "# Download dataset from Kaggle using KaggleHub\n",
        "path = kagglehub.dataset_download(\"carlosrunner/pizza-not-pizza\")\n",
        "\n",
        "# Path to the folder containing the classes directories\n",
        "data_path = os.path.join(path, os.listdir(path)[0])\n",
        "\n",
        "# List all image classes (subdirectories) in the dataset folder, sorted for consistency\n",
        "image_classes = sorted(os.listdir(data_path))[1:]\n",
        "\n",
        "# Print dataset location and classes found for confirmation\n",
        "print(\"Dataset directory:\", data_path)\n",
        "print(\"Classes found:\", image_classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9QBug6uqNLv",
        "outputId": "d9605bae-9820-4d53-8d03-740e86feaa11"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset directory: /kaggle/input/pizza-not-pizza/pizza_not_pizza\n",
            "Classes found: ['not_pizza', 'pizza']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================== #\n",
        "#      Constants and Configurations          #\n",
        "# ========================================== #\n",
        "\n",
        "# Features to be extracted from images\n",
        "FEATURES = ['gray', 'color', 'lbp', 'sobel']\n",
        "# FEATURES = ['gray']\n",
        "\n",
        "# Number of images to use from each class (can be limited for faster runs or place a huge value to use all the images)\n",
        "NUM_IMAGES_PER_CLASS = 100000\n",
        "\n",
        "# Size to which all images will be resized (width, height)\n",
        "IMAGE_SIZE = (128, 128)\n",
        "\n",
        "# Parameters for Local Binary Patterns (LBP)\n",
        "LBP_RADIUS = 1                  # Radius of circle for LBP\n",
        "LBP_N_POINTS = 8 * LBP_RADIUS  # Number of points to consider in LBP\n",
        "\n",
        "# Random state for reproducibility in splitting and models\n",
        "RANDOM_STATE = 0\n",
        "\n",
        "# Define default augmentation pipeline\n",
        "AUGMENTATION_PIPELINE = A.Compose([\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.RandomBrightnessContrast(p=0.2),\n",
        "    A.GaussianBlur(p=0.2),\n",
        "    A.Rotate(limit=15, p=0.3),\n",
        "])"
      ],
      "metadata": {
        "id": "V-BIMErYQe1C"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================== #\n",
        "#      Image Processing & Feature Utils      #\n",
        "# ========================================== #\n",
        "\n",
        "def load_and_resize_image(image_path, as_gray=True, augment=True):\n",
        "    \"\"\"\n",
        "    Load an image from disk, resize it, optionally apply augmentation, and return the result.\n",
        "\n",
        "    Parameters:\n",
        "    - image_path (str): Path to the image file.\n",
        "    - as_gray (bool): Load the image as grayscale if True; otherwise, as color.\n",
        "    - augment (bool): Whether to apply Albumentations image augmentation.\n",
        "\n",
        "    Returns:\n",
        "    - np.ndarray: The resized (and possibly augmented) image.\n",
        "    \"\"\"\n",
        "    image = imread(image_path, as_gray=as_gray)\n",
        "    image = resize(image, IMAGE_SIZE, anti_aliasing=True)\n",
        "\n",
        "    if augment:\n",
        "        # Convert to uint8 format required by Albumentations\n",
        "        image_uint8 = img_as_ubyte(image)\n",
        "\n",
        "        if as_gray:\n",
        "            # Albumentations expects grayscale images to have shape (H, W, 1)\n",
        "            image_uint8 = image_uint8[..., np.newaxis]\n",
        "\n",
        "        # Apply augmentation pipeline\n",
        "        augmented = AUGMENTATION_PIPELINE(image=image_uint8)\n",
        "        image_aug = augmented[\"image\"]\n",
        "\n",
        "        # Convert back to float32 normalized between 0 and 1\n",
        "        image_aug = image_aug.astype(np.float32) / 255.0\n",
        "\n",
        "        if as_gray:\n",
        "            image_aug = image_aug[..., 0]  # Remove channel dimension\n",
        "\n",
        "        return image_aug\n",
        "\n",
        "    return image\n",
        "\n",
        "def extract_color_histograms(image):\n",
        "    \"\"\"\n",
        "    Extract 256-bin histogram features for each RGB channel and concatenate.\n",
        "\n",
        "    Parameters:\n",
        "    - image (np.ndarray): Color image array.\n",
        "\n",
        "    Returns:\n",
        "    - np.ndarray: Concatenated histogram vector (length 256 * 3 = 768).\n",
        "    \"\"\"\n",
        "\n",
        "    # If image is grayscale, convert it to 3-channel by repeating it\n",
        "    if image.ndim == 2:\n",
        "        image = np.stack([image] * 3, axis=-1)\n",
        "\n",
        "    # For each color channel (R,G,B), compute histogram counts\n",
        "    histograms = [\n",
        "        np.histogram(image[:, :, channel], bins=256, range=(0, 256))[0]\n",
        "        for channel in range(3)\n",
        "    ]\n",
        "    # Concatenate histograms of all channels into one feature vector\n",
        "    concatenated_hist = np.concatenate(histograms)\n",
        "    return concatenated_hist\n",
        "\n",
        "def extract_lbp_features(gray_image):\n",
        "    \"\"\"\n",
        "    Compute Local Binary Pattern (LBP) histogram for a grayscale image.\n",
        "\n",
        "    Parameters:\n",
        "    - gray_image (np.ndarray): Grayscale image array.\n",
        "\n",
        "    Returns:\n",
        "    - np.ndarray: Normalized histogram of LBP patterns.\n",
        "    \"\"\"\n",
        "    # Convert grayscale image to unsigned 8-bit format (required by LBP)\n",
        "    image_uint8 = img_as_ubyte(gray_image)\n",
        "\n",
        "    # Compute LBP using 'uniform' method\n",
        "    lbp = local_binary_pattern(image_uint8, LBP_N_POINTS, LBP_RADIUS, method=\"uniform\")\n",
        "\n",
        "    # Define number of bins for histogram\n",
        "    n_bins = LBP_N_POINTS + 2  # Number of possible LBP patterns in uniform method\n",
        "\n",
        "    # Calculate normalized histogram of LBP\n",
        "    lbp_hist, _ = np.histogram(lbp, bins=n_bins, range=(0, n_bins), density=True)\n",
        "    return lbp_hist"
      ],
      "metadata": {
        "id": "D5kZwN9sm3Bp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================== #\n",
        "#          Dataset Feature Extraction        #\n",
        "# ========================================== #\n",
        "def extract_features_from_image(img_path, features, augment=False):\n",
        "    \"\"\"\n",
        "    Extract selected features from a single image and return concatenated vector.\n",
        "\n",
        "    Parameters:\n",
        "    - img_path (str): Path to the image file.\n",
        "    - features (list of str): Feature types to extract.\n",
        "    - augment (bool): Whether to apply augmentation.\n",
        "\n",
        "    Returns:\n",
        "    - np.ndarray: 1D vector of concatenated scaled features.\n",
        "    \"\"\"\n",
        "    feature_arrays = {f: [] for f in features}\n",
        "\n",
        "    # Load and resize gray image (augmentation handled inside)\n",
        "    gray = load_and_resize_image(img_path, as_gray=True, augment=augment)\n",
        "\n",
        "    if \"gray\" in features:\n",
        "        feature_arrays[\"gray\"].append(gray.flatten())\n",
        "\n",
        "    if \"color\" in features:\n",
        "        color_img = load_and_resize_image(img_path, as_gray=False, augment=augment)\n",
        "        feature_arrays[\"color\"].append(extract_color_histograms(color_img))\n",
        "\n",
        "    if \"lbp\" in features:\n",
        "        feature_arrays[\"lbp\"].append(extract_lbp_features(gray))\n",
        "\n",
        "    if \"sobel\" in features:\n",
        "        sobel_img = filters.sobel(gray)\n",
        "        feature_arrays[\"sobel\"].append(sobel_img.flatten())\n",
        "\n",
        "    if \"prewitt\" in features:\n",
        "        prewitt_img = filters.prewitt(gray)\n",
        "        feature_arrays[\"prewitt\"].append(prewitt_img.flatten())\n",
        "\n",
        "    arrays = []\n",
        "    for f in features:\n",
        "        array = np.array(feature_arrays[f])\n",
        "        arrays.append(array)\n",
        "\n",
        "    if len(arrays) > 1:\n",
        "        X = np.concatenate(arrays, axis=1)\n",
        "    else:\n",
        "        X = features[0]\n",
        "    return X\n",
        "\n",
        "\n",
        "\n",
        "def process_dataset_from_paths(image_paths, labels, features=[\"gray\"], augment=False):\n",
        "    \"\"\"\n",
        "    Load dataset images, extract selected features and encode labels.\n",
        "\n",
        "    Parameters:\n",
        "    - features (list of str): Feature types to extract.\n",
        "        Options include 'gray', 'color', 'lbp', 'sobel', 'prewitt'.\n",
        "\n",
        "    Returns:\n",
        "    - X (np.ndarray): Feature matrix of shape (num_samples, num_features).\n",
        "    - y (np.ndarray): Encoded integer labels for each sample.\n",
        "    \"\"\"\n",
        "    # Dictionary to hold lists of extracted features per feature type\n",
        "    feature_arrays = {f: [] for f in features}\n",
        "    processed_labels = []\n",
        "\n",
        "    print(f\"Total image paths received: {len(image_paths)}\")\n",
        "    print(f\"images are going to be resize to\", IMAGE_SIZE)\n",
        "\n",
        "    for idx, img_path in enumerate(image_paths):\n",
        "        label = os.path.basename(os.path.dirname(img_path))\n",
        "        processed_labels.append(label)\n",
        "\n",
        "        gray = load_and_resize_image(img_path, as_gray=True, augment=augment)\n",
        "\n",
        "        if \"gray\" in features:\n",
        "            feature_arrays[\"gray\"].append(gray.flatten())\n",
        "\n",
        "        if \"color\" in features:\n",
        "            color_img = load_and_resize_image(img_path, as_gray=False, augment=augment)\n",
        "            feature_arrays[\"color\"].append(extract_color_histograms(color_img))\n",
        "\n",
        "        if \"lbp\" in features:\n",
        "            feature_arrays[\"lbp\"].append(extract_lbp_features(gray))\n",
        "\n",
        "        if \"sobel\" in features:\n",
        "            sobel_img = filters.sobel(gray)\n",
        "            feature_arrays[\"sobel\"].append(sobel_img.flatten())\n",
        "\n",
        "        if \"prewitt\" in features:\n",
        "            prewitt_img = filters.prewitt(gray)\n",
        "            feature_arrays[\"prewitt\"].append(prewitt_img.flatten())\n",
        "\n",
        "        print(f\"\\rProcessed {idx + 1}/{len(image_paths)}\", end=\"\", flush=True)\n",
        "\n",
        "    print(\"\\nFinished feature extraction.\")\n",
        "\n",
        "    arrays = []\n",
        "    for f in features:\n",
        "        array = np.array(feature_arrays[f])\n",
        "        print(f\" Feature '{f}' extracted shape: {array.shape}\")\n",
        "        arrays.append(array)\n",
        "\n",
        "    if len(arrays) > 1:\n",
        "        X = np.concatenate(arrays, axis=1)\n",
        "    else:\n",
        "        X = features[0]\n",
        "\n",
        "    print(f\"Final feature matrix shape: {X.shape}\")\n",
        "\n",
        "    return X, processed_labels"
      ],
      "metadata": {
        "id": "fhkCY6AYd4m1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================== #\n",
        "#      Model Evaluation and Visualization    #\n",
        "# ========================================== #\n",
        "\n",
        "def plot_learning_curve(model, X, y, title=\"Learning Curve\", scoring=\"accuracy\", cv_splits=5):\n",
        "    \"\"\"\n",
        "    Plot the learning curve for a given model and dataset.\n",
        "\n",
        "    Parameters:\n",
        "    - model (estimator): A scikit-learn estimator (e.g., RandomForestClassifier).\n",
        "    - X (np.ndarray): Feature matrix.\n",
        "    - y (np.ndarray): Target labels.\n",
        "    - title (str): Title for the plot.\n",
        "    - scoring (str): Metric to evaluate model performance (default = 'accuracy').\n",
        "    - cv_splits (int): Number of cross-validation folds.\n",
        "    \"\"\"\n",
        "    # Create a stratified K-fold split strategy\n",
        "    cv = StratifiedKFold(n_splits=cv_splits, shuffle=True, random_state=RANDOM_STATE)\n",
        "\n",
        "    # Define relative sizes of training data to evaluate\n",
        "    train_sizes = np.linspace(0.05, 0.95, 19)\n",
        "\n",
        "    # Compute training and validation scores for different training sizes\n",
        "    train_sizes_abs, train_scores, val_scores = learning_curve(\n",
        "        estimator=model,\n",
        "        X=X,\n",
        "        y=y,\n",
        "        train_sizes=train_sizes,\n",
        "        cv=cv,\n",
        "        scoring=scoring,\n",
        "        n_jobs=-1,\n",
        "        shuffle=True,\n",
        "        random_state=RANDOM_STATE\n",
        "    )\n",
        "\n",
        "    # Compute mean and standard deviation for error bands\n",
        "    train_mean = np.mean(train_scores, axis=1)\n",
        "    train_std = np.std(train_scores, axis=1)\n",
        "    val_mean = np.mean(val_scores, axis=1)\n",
        "    val_std = np.std(val_scores, axis=1)\n",
        "\n",
        "    # Plot the learning curve\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(train_sizes_abs, train_mean, 'o-', label='Training Accuracy', color='blue')\n",
        "    plt.fill_between(train_sizes_abs, train_mean - train_std, train_mean + train_std, alpha=0.2, color='blue')\n",
        "\n",
        "    plt.plot(train_sizes_abs, val_mean, 'o-', label='Validation Accuracy', color='green')\n",
        "    plt.fill_between(train_sizes_abs, val_mean - val_std, val_mean + val_std, alpha=0.2, color='green')\n",
        "\n",
        "    # X-axis in percentage form for clarity\n",
        "    percent_labels = [f\"{int(p * 100)}%\" for p in train_sizes]\n",
        "    plt.xticks(train_sizes_abs, percent_labels, rotation=45)\n",
        "\n",
        "    # Set plot titles and labels\n",
        "    plt.title(title, fontsize=14, weight='bold')\n",
        "    plt.xlabel(\"Training Set Size (%)\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "vOU5j06qoGqm"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================== #\n",
        "#                  Training                  #\n",
        "# ========================================== #\n",
        "\n",
        "def train_grid_search(classifier, param_grid, X_train, X_test, y_train, y_test):\n",
        "    \"\"\"\n",
        "    Perform hyperparameter tuning with GridSearchCV on Classifier.\n",
        "\n",
        "    Parameters:\n",
        "    - classifier: estimator object (e.g. RandomForestClassifier)\n",
        "    - param_grid: dict, hyperparameters grid for GridSearchCV\n",
        "    - X (np.ndarray): Feature matrix.\n",
        "    - y (np.ndarray): Labels.\n",
        "\n",
        "    Returns:\n",
        "    - best_model (estimator): Best estimator after grid search.\n",
        "    \"\"\"\n",
        "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
        "    grid_search = GridSearchCV(\n",
        "        estimator=classifier,\n",
        "        param_grid=param_grid,\n",
        "        scoring='accuracy',\n",
        "        cv=cv,\n",
        "        verbose=3\n",
        "    )\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    print(\"Best hyperparameters found:\", grid_search.best_params_)\n",
        "    test_accuracy = accuracy_score(y_test, grid_search.best_estimator_.predict(X_test))\n",
        "    print(f\"Test accuracy with best model: {test_accuracy:.4f}\")\n",
        "    return grid_search.best_estimator_\n"
      ],
      "metadata": {
        "id": "SniKv_SKemBY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================== #\n",
        "#                Run Pipeline                #\n",
        "# ========================================== #\n",
        "\n",
        "# Step 1: List all image paths and labels\n",
        "image_paths = []\n",
        "labels = []\n",
        "\n",
        "for image_class in image_classes:\n",
        "    class_path = os.path.join(data_path, image_class)\n",
        "    image_list = os.listdir(class_path)[:NUM_IMAGES_PER_CLASS]\n",
        "    print(f\"Class '{image_class}': found {len(image_list)} images\")\n",
        "    for image_file in image_list:\n",
        "        image_paths.append(os.path.join(class_path, image_file))\n",
        "        labels.append(image_class)\n",
        "\n",
        "print(f\"Total images loaded: {len(image_paths)}\")\n",
        "\n",
        "# Step 2: Encode labels\n",
        "encoder = LabelEncoder()\n",
        "labels_encoded = encoder.fit_transform(labels)\n",
        "label_mapping = dict(zip(encoder.classes_, encoder.transform(encoder.classes_)))\n",
        "print(\"Label encoding mapping:\")\n",
        "for label, code in label_mapping.items():\n",
        "    print(f\" {label}: {code}\")\n",
        "\n",
        "# Step 3: Split into train/test before feature extraction\n",
        "train_paths, test_paths, y_train, y_test = train_test_split(\n",
        "    image_paths,\n",
        "    labels_encoded,\n",
        "    stratify=labels_encoded,\n",
        "    test_size=0.1,\n",
        "    shuffle=True,\n",
        "    random_state=RANDOM_STATE\n",
        ")\n",
        "print(f\"Training set: {len(train_paths)} images\")\n",
        "print(f\"Test set: {len(test_paths)} images\")\n",
        "\n",
        "# Step 4: Extract features separately\n",
        "print(\"\\nExtracting features for training set...\")\n",
        "X_train, y_train = process_dataset_from_paths(train_paths, y_train, features=FEATURES, augment=True)\n",
        "print(f\"X_train shape: {X_train.shape}, y_train length: {len(y_train)}\")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "\n",
        "print(\"\\nExtracting features for test set...\")\n",
        "X_test, y_test   = process_dataset_from_paths(test_paths, y_test, features=FEATURES, augment=False)\n",
        "print(f\"X_test shape: {X_test.shape}, y_test length: {len(y_test)}\")\n",
        "\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# PCA\n",
        "print(\"\\nFitting PCA on training features...\")\n",
        "pca = PCA(n_components=0.95, random_state=RANDOM_STATE)\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "print(f\"PCA components chosen: {pca.n_components_}, explained variance retained: {pca.explained_variance_ratio_.sum():.2f}\")\n",
        "print(f\"X_test_pca shape: {X_train_pca.shape}\")\n",
        "\n",
        "print(\"Transforming test features with PCA...\")\n",
        "X_test_pca  = pca.transform(X_test)\n",
        "print(f\"X_test_pca shape: {X_test_pca.shape}\")\n",
        "\n",
        "\n",
        "# Get full X and y to plot learning curve\n",
        "X = np.concatenate([X_train_pca, X_test_pca], axis=0)\n",
        "y = np.concatenate([y_train, y_test], axis=0)\n",
        "print(f\"X shape: {X.shape}\")\n",
        "print(f\"y shape: {y.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8r0IJt3_pBHZ",
        "outputId": "d3d301a9-7552-44fc-99ba-8a770c8a3140"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class 'not_pizza': found 983 images\n",
            "Class 'pizza': found 983 images\n",
            "Total images loaded: 1966\n",
            "Label encoding mapping:\n",
            " not_pizza: 0\n",
            " pizza: 1\n",
            "Training set: 1769 images\n",
            "Test set: 197 images\n",
            "\n",
            "Extracting features for training set...\n",
            "Total image paths received: 1769\n",
            "images are going to be resize to (128, 128)\n",
            "Processed 1769/1769\n",
            "Finished feature extraction.\n",
            "Feature 'gray' extracted shape: (1769, 16384)\n",
            "Feature 'color' extracted shape: (1769, 768)\n",
            "Feature 'lbp' extracted shape: (1769, 10)\n",
            "Feature 'sobel' extracted shape: (1769, 16384)\n",
            "Final feature matrix shape: (1769, 33546)\n",
            "X_train shape: (1769, 33546), y_train length: 1769\n",
            "\n",
            "Extracting features for test set...\n",
            "Total image paths received: 197\n",
            "images are going to be resize to (128, 128)\n",
            "Processed 197/197\n",
            "Finished feature extraction.\n",
            "Feature 'gray' extracted shape: (197, 16384)\n",
            "Feature 'color' extracted shape: (197, 768)\n",
            "Feature 'lbp' extracted shape: (197, 10)\n",
            "Feature 'sobel' extracted shape: (197, 16384)\n",
            "Final feature matrix shape: (197, 33546)\n",
            "X_test shape: (197, 33546), y_test length: 197\n",
            "\n",
            "Fitting PCA on training features...\n",
            "PCA components chosen: 1028, explained variance retained: 0.95\n",
            "X_test_pca shape: (1769, 1028)\n",
            "Transforming test features with PCA...\n",
            "X_test_pca shape: (197, 1028)\n",
            "X shape: (1966, 1028)\n",
            "y shape: (1966,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.save('X_train.npy', X_train)\n",
        "np.save('X_test.npy', X_test)\n",
        "np.save('y_train.npy', y_train)\n",
        "np.save('y_test.npy', y_test)"
      ],
      "metadata": {
        "id": "wCSVIJQFrOVp"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = {\n",
        "    \"Random Forest\": RandomForestClassifier(random_state=RANDOM_STATE, class_weight=\"balanced\"),\n",
        "    \"K-Nearest Neighbors\": KNeighborsClassifier(),\n",
        "    \"Support Vector Classifier\": SVC(),\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(random_state=RANDOM_STATE, class_weight=\"balanced\"),\n",
        "    \"Naive Bayes\": GaussianNB()\n",
        "}\n",
        "\n",
        "best_models = {\n",
        "    \"not-pca\": {\"score\": 0, \"clf\": None, \"name\": \"\"},\n",
        "    \"pca\": {\"score\": 0, \"clf\": None, \"name\": \"\"}\n",
        "}\n",
        "\n",
        "for use_pca in [False, True]:\n",
        "    key = \"pca\" if use_pca else \"not-pca\"\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 40)\n",
        "    print(\"Using PCA\" if use_pca else \"Using original features\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "    X_tr = X_train_pca if use_pca else X_train\n",
        "    X_te = X_test_pca if use_pca else X_test\n",
        "\n",
        "    for name, model in models.items():\n",
        "        clf = clone(model)\n",
        "        clf.fit(X_tr, y_train)\n",
        "\n",
        "        y_pred_train = clf.predict(X_tr)\n",
        "        y_pred_test = clf.predict(X_te)\n",
        "\n",
        "        print(f\"\\n--- {name} ---\")\n",
        "        print(\"Train Metrics\")\n",
        "        print(classification_report(y_train, y_pred_train))\n",
        "        print(\"Test Metrics\")\n",
        "        print(classification_report(y_test, y_pred_test))\n",
        "\n",
        "        scores = cross_val_score(clf, X_tr, y_train, cv=5, scoring='accuracy')\n",
        "        mean_score = scores.mean()\n",
        "        print(\"CV train scores:\", scores, \"→ mean:\", mean_score)\n",
        "\n",
        "        if mean_score > best_models[key][\"score\"]:\n",
        "            best_models[key] = {\n",
        "                \"score\": mean_score,\n",
        "                \"clf\": clf,\n",
        "                \"name\": f\"{name} {key}\"\n",
        "            }\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\">> Best model WITHOUT PCA:\")\n",
        "print(f\"{best_models['not-pca']['name']} with mean CV accuracy: {best_models['not-pca']['score']:.4f}\")\n",
        "\n",
        "print(\"\\n>> Best model WITH PCA:\")\n",
        "print(f\"{best_models['pca']['name']} with mean CV accuracy: {best_models['pca']['score']:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sr740MJ0nEsV",
        "outputId": "afcf70bf-cac2-491a-b223-f7ee9d312792"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========================================\n",
            "Using original features\n",
            "========================================\n",
            "\n",
            "--- Random Forest ---\n",
            "Train Metrics\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   not_pizza       1.00      1.00      1.00       884\n",
            "       pizza       1.00      1.00      1.00       885\n",
            "\n",
            "    accuracy                           1.00      1769\n",
            "   macro avg       1.00      1.00      1.00      1769\n",
            "weighted avg       1.00      1.00      1.00      1769\n",
            "\n",
            "Test Metrics\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   not_pizza       0.76      0.73      0.74        99\n",
            "       pizza       0.74      0.77      0.75        98\n",
            "\n",
            "    accuracy                           0.75       197\n",
            "   macro avg       0.75      0.75      0.75       197\n",
            "weighted avg       0.75      0.75      0.75       197\n",
            "\n",
            "CV train scores: [0.71751412 0.64971751 0.7259887  0.68926554 0.73937677] → mean: 0.7043725292488917\n",
            "\n",
            "--- K-Nearest Neighbors ---\n",
            "Train Metrics\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   not_pizza       0.85      0.46      0.59       884\n",
            "       pizza       0.63      0.92      0.75       885\n",
            "\n",
            "    accuracy                           0.69      1769\n",
            "   macro avg       0.74      0.69      0.67      1769\n",
            "weighted avg       0.74      0.69      0.67      1769\n",
            "\n",
            "Test Metrics\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   not_pizza       0.73      0.27      0.40        99\n",
            "       pizza       0.55      0.90      0.68        98\n",
            "\n",
            "    accuracy                           0.58       197\n",
            "   macro avg       0.64      0.59      0.54       197\n",
            "weighted avg       0.64      0.58      0.54       197\n",
            "\n",
            "CV train scores: [0.55649718 0.55084746 0.57344633 0.5480226  0.59773371] → mean: 0.5653094540740385\n",
            "\n",
            "--- Support Vector Classifier ---\n",
            "Train Metrics\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   not_pizza       0.93      0.94      0.93       884\n",
            "       pizza       0.94      0.93      0.93       885\n",
            "\n",
            "    accuracy                           0.93      1769\n",
            "   macro avg       0.93      0.93      0.93      1769\n",
            "weighted avg       0.93      0.93      0.93      1769\n",
            "\n",
            "Test Metrics\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   not_pizza       0.74      0.71      0.73        99\n",
            "       pizza       0.72      0.76      0.74        98\n",
            "\n",
            "    accuracy                           0.73       197\n",
            "   macro avg       0.73      0.73      0.73       197\n",
            "weighted avg       0.73      0.73      0.73       197\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for mode in ['pca', 'not-pca']:\n",
        "  plot_learning_curve(best_models[mode][\"clf\"], X, y, f\"{best_models[mode]['name']} Learning curve\")"
      ],
      "metadata": {
        "id": "WljfQXwfxCX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_images_to_show = 50\n",
        "rows = num_images_to_show // 10\n",
        "cols = 10\n",
        "\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(cols * 2, rows * 2))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i in range(num_images_to_show):\n",
        "    rand_idx = np.random.randint(len(image_paths))\n",
        "    image = imread(image_paths[rand_idx])\n",
        "\n",
        "    x = scaler.transform(extract_features_from_image(image_paths[rand_idx], FEATURES))\n",
        "    x = pca.transform(x)\n",
        "    pred = best_models[\"pca\"][clf].predict(x)[0]\n",
        "    true = labels[rand_idx]\n",
        "    color = 'green' if pred == true else 'red'\n",
        "\n",
        "    axes[i].imshow(image)\n",
        "    axes[i].axis('off')\n",
        "    axes[i].set_title(f\"Predicted: {pred}\\nActual: {true}\", fontsize=12, color=color)\n",
        "plt.title(f\"{best_models[\"pca\"][\"name\"]}\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WN4iXlBbc8e1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}